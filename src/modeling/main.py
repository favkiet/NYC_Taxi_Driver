import pandas as pd
import numpy as np
import xgboost as xgb
import pickle
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

class TaxiDemandPredictor:
    def __init__(self):
        self.model = None

    def load_data(self, s3_path, storage_options):
        """Äá»c dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ tá»« MinIO vá» Pandas"""
        # storage_options giÃºp Pandas Ä‘á»c trá»±c tiáº¿p s3
        df = pd.read_parquet(s3_path, storage_options=storage_options)
        return df
    
    def load_data_multiple_sources(self, batch_path, streaming_path, storage_options):
        """
        Äá»c vÃ  merge dá»¯ liá»‡u tá»« cáº£ batch vÃ  streaming sources
        
        Args:
            batch_path: ÄÆ°á»ng dáº«n batch data (s3://bucket/processed/taxi_demand_features)
            streaming_path: ÄÆ°á»ng dáº«n streaming data (s3://bucket/streaming/demand_aggregated)
            storage_options: MinIO connection options
        """
        print("ğŸ“¥ Äang táº£i dá»¯ liá»‡u tá»« nhiá»u nguá»“n...")
        
        # Load batch data
        try:
            batch_df = pd.read_parquet(batch_path, storage_options=storage_options)
            print(f"âœ“ Batch data: {len(batch_df):,} records")
        except Exception as e:
            print(f"âš  KhÃ´ng Ä‘á»c Ä‘Æ°á»£c batch data: {e}")
            batch_df = pd.DataFrame()
        
        # Load streaming data
        try:
            streaming_df = pd.read_parquet(streaming_path, storage_options=storage_options)
            print(f"âœ“ Streaming data: {len(streaming_df):,} records")
            
            # Náº¿u streaming cÃ³ window column, extract date_str tá»« window
            if 'window' in streaming_df.columns:
                # Window format: [2025-01-04 16:30:00, 2025-01-04 16:31:00)
                # Extract start time vÃ  convert to date
                streaming_df['date_str'] = pd.to_datetime(streaming_df['window'].str.split(',').str[0].str.strip('['))
                streaming_df['date_str'] = streaming_df['date_str'].dt.date
            
            # Náº¿u streaming data chÆ°a cÃ³ aggregation (thiáº¿u trip_count, avg_distance)
            # ThÃ¬ aggregate tá»« raw data
            if 'trip_count' not in streaming_df.columns or 'avg_distance' not in streaming_df.columns:
                print("  â†’ Aggregating streaming data...")
                # Kiá»ƒm tra cÃ³ Ä‘á»§ columns Ä‘á»ƒ aggregate khÃ´ng
                if all(col in streaming_df.columns for col in ['date_str', 'pickup_hour', 'PULocationID', 'trip_distance']):
                    # Aggregate: count trips vÃ  sum distance
                    streaming_df = streaming_df.groupby(['date_str', 'pickup_hour', 'PULocationID']).agg({
                        'trip_distance': ['count', 'sum']  # count = trip_count, sum = avg_distance
                    }).reset_index()
                    streaming_df.columns = ['date_str', 'pickup_hour', 'PULocationID', 'trip_count', 'avg_distance']
                    print(f"  âœ“ Aggregated to {len(streaming_df):,} records")
                else:
                    print("âš  Streaming data thiáº¿u columns Ä‘á»ƒ aggregate, bá» qua...")
                    streaming_df = pd.DataFrame()
            else:
                # Äáº£m báº£o columns giá»‘ng batch
                required_cols = ['date_str', 'pickup_hour', 'PULocationID', 'trip_count', 'avg_distance']
                if not all(col in streaming_df.columns for col in required_cols):
                    print("âš  Streaming data thiáº¿u columns, bá» qua...")
                    streaming_df = pd.DataFrame()
        except Exception as e:
            print(f"âš  KhÃ´ng Ä‘á»c Ä‘Æ°á»£c streaming data: {e}")
            streaming_df = pd.DataFrame()
        
        # Merge 2 dataframes
        if not batch_df.empty and not streaming_df.empty:
            # Combine vÃ  remove duplicates (náº¿u cÃ³ overlap)
            combined_df = pd.concat([batch_df, streaming_df], ignore_index=True)
            # Group by Ä‘á»ƒ aggregate náº¿u cÃ³ duplicate (date_str, pickup_hour, PULocationID)
            combined_df = combined_df.groupby(['date_str', 'pickup_hour', 'PULocationID']).agg({
                'trip_count': 'sum',
                'avg_distance': 'sum'
            }).reset_index()
            print(f"âœ“ Combined data: {len(combined_df):,} records")
            return combined_df
        elif not batch_df.empty:
            return batch_df
        elif not streaming_df.empty:
            return streaming_df
        else:
            raise ValueError("KhÃ´ng cÃ³ dá»¯ liá»‡u tá»« cáº£ 2 nguá»“n!")

    def train(self, df):
        # Chá»n features vÃ  target
        # Features: Giá», Khu vá»±c. Target: Sá»‘ chuyáº¿n xe
        X = df[['pickup_hour', 'PULocationID']] 
        y = df['trip_count']

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        print("Äang train XGBoost Model...")
        self.model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)
        self.model.fit(X_train, y_train)
        
        # ÄÃ¡nh giÃ¡ sÆ¡ bá»™
        predictions = self.model.predict(X_test)
        mse = mean_squared_error(y_test, predictions)
        rmse = np.sqrt(mse)
        print(f"Model RMSE: {rmse:.2f}")
        return self.model

    def save_model(self, path="model.pkl"):
        with open(path, "wb") as f:
            pickle.dump(self.model, f)
        print(f"Model Ä‘Ã£ lÆ°u táº¡i: {path}")