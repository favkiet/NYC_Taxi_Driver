import pandas as pd
import numpy as np
import xgboost as xgb
import pickle
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

class TaxiDemandPredictor:
    def __init__(self):
        self.model = None
        self.feature_names = ['day_of_week', 'month', 'is_weekend', 'pickup_hour', 'PULocationID']

    def load_data(self, s3_path, storage_options):
        """Äá»c dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ tá»« MinIO vá» Pandas"""
        # storage_options giÃºp Pandas Ä‘á»c trá»±c tiáº¿p s3
        df = pd.read_parquet(s3_path, storage_options=storage_options)
        return df
    
    def load_data_multiple_sources(self, batch_path, streaming_path, storage_options):
        """
        Äá»c vÃ  merge dá»¯ liá»‡u tá»« cáº£ batch vÃ  streaming sources
        
        Args:
            batch_path: ÄÆ°á»ng dáº«n batch data (s3://bucket/processed/taxi_demand_features)
            streaming_path: ÄÆ°á»ng dáº«n streaming data (s3://bucket/streaming/demand_aggregated)
            storage_options: MinIO connection options
        """
        print("ğŸ“¥ Äang táº£i dá»¯ liá»‡u tá»« nhiá»u nguá»“n...")
        
        # Load batch data
        try:
            batch_df = pd.read_parquet(batch_path, storage_options=storage_options)
            print(f"âœ“ Batch data: {len(batch_df):,} records")
        except Exception as e:
            print(f"âš  KhÃ´ng Ä‘á»c Ä‘Æ°á»£c batch data: {e}")
            batch_df = pd.DataFrame()
        
        # Load streaming data
        try:
            streaming_df = pd.read_parquet(streaming_path, storage_options=storage_options)
            print(f"âœ“ Streaming data: {len(streaming_df):,} records")
            
            # Náº¿u streaming cÃ³ window column, extract date_str tá»« window
            if 'window' in streaming_df.columns:
                # Window format: [2025-01-04 16:30:00, 2025-01-04 16:31:00)
                # Extract start time vÃ  convert to date
                streaming_df['date_str'] = pd.to_datetime(streaming_df['window'].str.split(',').str[0].str.strip('['))
                streaming_df['date_str'] = streaming_df['date_str'].dt.date
            
            # Náº¿u streaming data chÆ°a cÃ³ aggregation (thiáº¿u trip_count, avg_distance)
            # ThÃ¬ aggregate tá»« raw data
            if 'trip_count' not in streaming_df.columns or 'avg_distance' not in streaming_df.columns:
                print("  â†’ Aggregating streaming data...")
                # Kiá»ƒm tra cÃ³ Ä‘á»§ columns Ä‘á»ƒ aggregate khÃ´ng
                if all(col in streaming_df.columns for col in ['date_str', 'pickup_hour', 'PULocationID', 'trip_distance']):
                    # Aggregate: count trips vÃ  sum distance
                    streaming_df = streaming_df.groupby(['date_str', 'pickup_hour', 'PULocationID']).agg({
                        'trip_distance': ['count', 'sum']  # count = trip_count, sum = avg_distance
                    }).reset_index()
                    streaming_df.columns = ['date_str', 'pickup_hour', 'PULocationID', 'trip_count', 'avg_distance']
                    print(f"  âœ“ Aggregated to {len(streaming_df):,} records")
                else:
                    print("âš  Streaming data thiáº¿u columns Ä‘á»ƒ aggregate, bá» qua...")
                    streaming_df = pd.DataFrame()
            else:
                # Äáº£m báº£o columns giá»‘ng batch
                required_cols = ['date_str', 'pickup_hour', 'PULocationID', 'trip_count', 'avg_distance']
                if not all(col in streaming_df.columns for col in required_cols):
                    print("âš  Streaming data thiáº¿u columns, bá» qua...")
                    streaming_df = pd.DataFrame()
        except Exception as e:
            print(f"âš  KhÃ´ng Ä‘á»c Ä‘Æ°á»£c streaming data: {e}")
            streaming_df = pd.DataFrame()
        
        # Merge 2 dataframes
        if not batch_df.empty and not streaming_df.empty:
            # Combine vÃ  remove duplicates (náº¿u cÃ³ overlap)
            combined_df = pd.concat([batch_df, streaming_df], ignore_index=True)
            # Group by Ä‘á»ƒ aggregate náº¿u cÃ³ duplicate (date_str, pickup_hour, PULocationID)
            combined_df = combined_df.groupby(['date_str', 'pickup_hour', 'PULocationID']).agg({
                'trip_count': 'sum',
                'avg_distance': 'sum'
            }).reset_index()
            print(f"âœ“ Combined data: {len(combined_df):,} records")
            return combined_df
        elif not batch_df.empty:
            return batch_df
        elif not streaming_df.empty:
            return streaming_df
        else:
            raise ValueError("KhÃ´ng cÃ³ dá»¯ liá»‡u tá»« cáº£ 2 nguá»“n!")

    def extract_date_features(self, df):
        """
        Extract features tá»« date_str: day_of_week, month, is_weekend
        """
        # Äáº£m báº£o date_str lÃ  datetime
        # Xá»­ lÃ½ cÃ¡c trÆ°á»ng há»£p: date object, string, hoáº·c datetime
        if df['date_str'].dtype == 'object':
            # CÃ³ thá»ƒ lÃ  string hoáº·c date object
            df['date_str'] = pd.to_datetime(df['date_str'])
        elif not pd.api.types.is_datetime64_any_dtype(df['date_str']):
            # Náº¿u khÃ´ng pháº£i datetime, convert
            df['date_str'] = pd.to_datetime(df['date_str'])
        
        # Extract features tá»« date
        df['day_of_week'] = df['date_str'].dt.dayofweek  # 0=Monday, 6=Sunday
        df['month'] = df['date_str'].dt.month  # 1-12
        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)  # Saturday=5, Sunday=6
        
        return df

    def train(self, df):
        """
        Train model vá»›i features: date_str (extracted), pickup_hour, PULocationID
        Target: trip_count
        """
        print("\nğŸ”§ Äang extract features tá»« date_str...")
        df = self.extract_date_features(df.copy())
        
        # Chá»n features vÃ  target
        # Features: day_of_week, month, is_weekend, pickup_hour, PULocationID
        feature_cols = ['day_of_week', 'month', 'is_weekend', 'pickup_hour', 'PULocationID']
        
        # Kiá»ƒm tra columns cÃ³ tá»“n táº¡i khÃ´ng
        missing_cols = [col for col in feature_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Thiáº¿u columns: {missing_cols}")
        
        X = df[feature_cols]
        y = df['trip_count']
        
        print(f"âœ“ Features sá»­ dá»¥ng: {feature_cols}")
        print(f"âœ“ Sá»‘ lÆ°á»£ng samples: {len(X):,}")
        print(f"âœ“ Target: trip_count (min={y.min():.0f}, max={y.max():.0f}, mean={y.mean():.2f})")

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        print("\nğŸš€ Äang train XGBoost Model...")
        self.model = xgb.XGBRegressor(
            objective='reg:squarederror', 
            n_estimators=100,
            random_state=42
        )
        self.model.fit(X_train, y_train)
        
        # ÄÃ¡nh giÃ¡ sÆ¡ bá»™
        predictions = self.model.predict(X_test)
        mse = mean_squared_error(y_test, predictions)
        rmse = np.sqrt(mse)
        print(f"âœ“ Model RMSE: {rmse:.2f}")
        print(f"âœ“ Model MAE: {np.mean(np.abs(y_test - predictions)):.2f}")
        
        # LÆ°u feature names Ä‘á»ƒ API sá»­ dá»¥ng
        self.feature_names = feature_cols
        
        return self.model

    def save_model(self, path="model.pkl"):
        """
        LÆ°u model vÃ  feature names
        """
        model_data = {
            'model': self.model,
            'feature_names': self.feature_names
        }
        with open(path, "wb") as f:
            pickle.dump(model_data, f)
        print(f"Model Ä‘Ã£ lÆ°u táº¡i: {path}")
        print(f"Features: {self.feature_names}")