from pyspark.sql import SparkSession
from pyspark.sql.functions import col, hour, dayofweek, month, to_timestamp, count, sum as _sum, when, coalesce

class TaxiDataProcessor:
    def __init__(self, spark: SparkSession):
        self.spark = spark

    def read_from_minio(self, bucket_name: str, folders: list):
        """
        ƒê·ªçc d·ªØ li·ªáu t·ª´ nhi·ªÅu folder kh√°c nhau tr√™n MinIO (Yellow v√† Green Taxi)
        """
        full_paths = [
            "s3a://nyc-taxi-driver/raw/2025/*.parquet",  # Yellow Taxi
        ]
        print(f"\n{'='*60}")
        print("üì• B∆Ø·ªöC 1: ƒê·ªåC D·ªÆ LI·ªÜU T·ª™ MINIO")
        print(f"{'='*60}")
        print("ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´:")
        for path in full_paths:
            print(f"  - {path}")
        
        # option("mergeSchema", "true") ƒë·ªÉ g·ªôp schema n·∫øu 2 lo·∫°i taxi l·ªách nhau
        df = self.spark.read.option("mergeSchema", "true").parquet(*full_paths)
        
        total_records = df.count()
        print(f"\n‚úì ƒê√£ ƒë·ªçc th√†nh c√¥ng: {total_records:,} records")
        
        # Ki·ªÉm tra lo·∫°i taxi
        has_yellow = "tpep_pickup_datetime" in df.columns
        has_green = "lpep_pickup_datetime" in df.columns
        
        if has_yellow and has_green:
            yellow_count = df.filter(col("tpep_pickup_datetime").isNotNull()).count()
            green_count = df.filter(col("lpep_pickup_datetime").isNotNull()).count()
            print(f"  - Yellow Taxi: {yellow_count:,} records")
            print(f"  - Green Taxi: {green_count:,} records")
        elif has_yellow:
            print("  - Ch·ªâ c√≥ Yellow Taxi")
        elif has_green:
            print("  - Ch·ªâ c√≥ Green Taxi")
        
        print(f"Schema c√≥ {len(df.columns)} c·ªôt")
        return df

    def clean_and_engineer(self, df):
        """
        B∆∞·ªõc 1 & 2: L√†m s·∫°ch + Feature Engineering (Time & Zone)
        X·ª≠ l√Ω c·∫£ Yellow Taxi (tpep_pickup_datetime) v√† Green Taxi (lpep_pickup_datetime)
        """
        print("\n{'='*60}")
        print("üßπ B∆Ø·ªöC 2: L√ÄM S·∫†CH V√Ä FEATURE ENGINEERING")
        print(f"{'='*60}")
        
        initial_count = df.count()
        print(f"Records ban ƒë·∫ßu: {initial_count:,}")
        
        # T·∫°o c·ªôt pickup_datetime th·ªëng nh·∫•t t·ª´ c·∫£ 2 lo·∫°i taxi
        pickup_datetime = coalesce(
            col("tpep_pickup_datetime"),  # Yellow Taxi
            col("lpep_pickup_datetime")   # Green Taxi
        )
        
        # L·ªçc d·ªØ li·ªáu r√°c
        print("\nƒêang l·ªçc d·ªØ li·ªáu:")
        print("  - Lo·∫°i b·ªè records thi·∫øu pickup_datetime")
        df_after_datetime = df.filter(pickup_datetime.isNotNull())
        print("  - Lo·∫°i b·ªè records c√≥ trip_distance <= 0")
        df_after_distance = df_after_datetime.filter(col("trip_distance") > 0)
        print("  - Lo·∫°i b·ªè records thi·∫øu PULocationID")
        df_clean = df_after_distance.filter(col("PULocationID").isNotNull())
        
        cleaned_count = df_clean.count()
        removed_count = initial_count - cleaned_count
        print(f"\n‚úì Sau khi l√†m s·∫°ch: {cleaned_count:,} records (ƒë√£ lo·∫°i b·ªè {removed_count:,} records)")
        
        # Feature Engineering: T√°ch gi·ªù, th·ª©, ng√†y th√°ng
        print(f"\nƒêang t·∫°o features:")
        print("  - pickup_datetime (th·ªëng nh·∫•t t·ª´ Yellow/Green)")
        print("  - pickup_hour, pickup_day, pickup_month")
        print("  - date_str")
        
        df_featured = df_clean.withColumn("pickup_datetime", pickup_datetime) \
                            .withColumn("pickup_hour", hour(pickup_datetime)) \
                            .withColumn("pickup_day", dayofweek(pickup_datetime)) \
                            .withColumn("pickup_month", month(pickup_datetime)) \
                            .withColumn("date_str", pickup_datetime.cast("date"))
        
        print("‚úì Ho√†n th√†nh feature engineering")
        return df_featured

    def aggregate_demand(self, df):
        """
        B∆∞·ªõc 3: T·ªïng h·ª£p nhu c·∫ßu (Demand Aggregation)
        Output: M·ªói d√≤ng l√† 1 Khu v·ª±c - 1 Khung gi·ªù - S·ªë l∆∞·ª£ng chuy·∫øn
        """
        print(f"\n{'='*60}")
        print("üìä B∆Ø·ªöC 3: T·ªîNG H·ª¢P NHU C·∫¶U")
        print(f"{'='*60}")
        
        input_count = df.count()
        print(f"Records ƒë·∫ßu v√†o: {input_count:,}")
        
        print("\nƒêang t·ªïng h·ª£p theo:")
        print("  - date_str (ng√†y)")
        print("  - pickup_hour (gi·ªù)")
        print("  - PULocationID (khu v·ª±c)")
        print(f"\nT√≠nh to√°n:")
        print("  - trip_count: s·ªë l∆∞·ª£ng chuy·∫øn")
        print("  - avg_distance: t·ªïng qu√£ng ƒë∆∞·ªùng")
        
        df_agg = df.groupBy("date_str", "pickup_hour", "PULocationID") \
                    .agg(
                        count("VendorID").alias("trip_count"), # S·ªë l∆∞·ª£ng xe ƒë√≥n (Demand)
                        _sum("trip_distance").alias("avg_distance")
                    )
        
        output_count = df_agg.count()
        print(f"\n‚úì Sau khi t·ªïng h·ª£p: {output_count:,} records")
        print("  (M·ªói record = 1 khu v·ª±c x 1 khung gi·ªù x 1 ng√†y)")
        
        return df_agg